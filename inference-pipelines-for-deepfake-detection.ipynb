{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af60d56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T09:35:30.959579Z",
     "iopub.status.busy": "2025-07-20T09:35:30.959272Z",
     "iopub.status.idle": "2025-07-20T09:35:46.544957Z",
     "shell.execute_reply": "2025-07-20T09:35:46.544401Z"
    },
    "papermill": {
     "duration": 15.591132,
     "end_time": "2025-07-20T09:35:46.546177",
     "exception": false,
     "start_time": "2025-07-20T09:35:30.955045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - ============================================================\n",
      "INFO - DEEPFAKE DETECTION INFERENCE PIPELINE INITIALIZED\n",
      "INFO - ============================================================\n",
      "INFO - Device: cuda\n",
      "INFO - Models directory: /kaggle/input/all-models-trained\n",
      "INFO - Log file: /kaggle/working/logs/deepfake_inference_20250720_093546.log\n",
      "INFO - GPU: Tesla T4, Memory: 14 GB\n",
      "INFO - Transform pipelines initialized\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports, Configuration & Logging Setup\n",
    "import os, sys\n",
    "import logging\n",
    "import argparse\n",
    "import torch, torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import librosa\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "def setup_logging(log_level=\"INFO\", log_file=None):\n",
    "    \"\"\"Setup comprehensive logging configuration\"\"\"\n",
    "    # Create logs directory\n",
    "    log_dir = \"/kaggle/working/logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Default log file with timestamp\n",
    "    if log_file is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = os.path.join(log_dir, f\"deepfake_inference_{timestamp}.log\")\n",
    "    \n",
    "    # Configure logging format\n",
    "    log_format = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # Setup file handler\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(log_format)\n",
    "    \n",
    "    # Setup console handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(getattr(logging, log_level))\n",
    "    console_format = logging.Formatter('%(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_format)\n",
    "    \n",
    "    # Configure root logger\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.DEBUG)\n",
    "    root_logger.handlers = []  # Clear existing handlers\n",
    "    root_logger.addHandler(file_handler)\n",
    "    root_logger.addHandler(console_handler)\n",
    "    \n",
    "    return log_file\n",
    "\n",
    "# Initialize logging\n",
    "LOG_FILE = setup_logging(\"INFO\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"models_dir\": \"/kaggle/input/all-models-trained\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"img_size\": (224, 224),\n",
    "    \"audio_sr\": 16000,\n",
    "    \"audio_n_mels\": 128,\n",
    "    \"audio_duration\": 2.5,\n",
    "    \"video_max_frames\": 8,\n",
    "    \"batch_size\": 32,\n",
    "    \"inference_timeout\": 30  # seconds\n",
    "}\n",
    "\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"DEEPFAKE DETECTION INFERENCE PIPELINE INITIALIZED\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(f\"Device: {CONFIG['device']}\")\n",
    "logger.info(f\"Models directory: {CONFIG['models_dir']}\")\n",
    "logger.info(f\"Log file: {LOG_FILE}\")\n",
    "\n",
    "# Verify GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    logger.info(f\"GPU: {gpu_info.name}, Memory: {gpu_info.total_memory // 1024**3} GB\")\n",
    "else:\n",
    "    logger.warning(\"GPU not available, using CPU (inference will be slower)\")\n",
    "\n",
    "# Common transforms\n",
    "IMAGE_TRANSFORM = T.Compose([\n",
    "    T.Resize(CONFIG[\"img_size\"]),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "VIDEO_TRANSFORM = IMAGE_TRANSFORM  # Apply per-frame\n",
    "logger.info(\"Transform pipelines initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153f7394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T09:35:46.553725Z",
     "iopub.status.busy": "2025-07-20T09:35:46.553428Z",
     "iopub.status.idle": "2025-07-20T09:35:46.565142Z",
     "shell.execute_reply": "2025-07-20T09:35:46.564346Z"
    },
    "papermill": {
     "duration": 0.016053,
     "end_time": "2025-07-20T09:35:46.566246",
     "exception": false,
     "start_time": "2025-07-20T09:35:46.550193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Model classes defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Enhanced Model Definitions with Logging\n",
    "def safe_efficientnet(pretrained=False):\n",
    "    \"\"\"Load EfficientNet with fallback and logging\"\"\"\n",
    "    try:\n",
    "        logger.debug(f\"Loading EfficientNet with pretrained={pretrained}\")\n",
    "        model = torch.hub.load('pytorch/vision:v0.13.1', 'efficientnet_b0', \n",
    "                              pretrained=pretrained, verbose=False)\n",
    "        logger.debug(\"EfficientNet loaded successfully\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load EfficientNet: {e}\")\n",
    "        logger.warning(\"Falling back to random initialization\")\n",
    "        return torch.hub.load('pytorch/vision:v0.13.1', 'efficientnet_b0', \n",
    "                             pretrained=False, verbose=False)\n",
    "\n",
    "class ImageModel(nn.Module):\n",
    "    \"\"\"Image deepfake detection model\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        logger.debug(\"Initializing ImageModel\")\n",
    "        m = safe_efficientnet(False)\n",
    "        m.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "        self.net = m\n",
    "        logger.debug(\"ImageModel initialized successfully\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class VideoModel(nn.Module):\n",
    "    \"\"\"Video deepfake detection model\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        logger.debug(\"Initializing VideoModel\")\n",
    "        m = safe_efficientnet(False)\n",
    "        m.classifier = nn.Identity()\n",
    "        self.back = m\n",
    "        self.lstm = nn.LSTM(1280, 256, 2, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "        logger.debug(\"VideoModel initialized successfully\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, f, c, h, w = x.shape\n",
    "        logger.debug(f\"Video input shape: {x.shape}\")\n",
    "        feats = self.back(x.view(b*f, c, h, w)).view(b, f, -1)\n",
    "        _, (h_n, _) = self.lstm(feats)\n",
    "        return self.fc(h_n[-1])\n",
    "\n",
    "class AudioModel(nn.Module):\n",
    "    \"\"\"Audio deepfake detection model\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        logger.debug(\"Initializing AudioModel\")\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1)), nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        logger.debug(\"AudioModel initialized successfully\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logger.debug(f\"Audio input shape: {x.shape}\")\n",
    "        return self.fc(self.cnn(x))\n",
    "\n",
    "logger.info(\"Model classes defined successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4827eb7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T09:35:46.572414Z",
     "iopub.status.busy": "2025-07-20T09:35:46.572143Z",
     "iopub.status.idle": "2025-07-20T09:35:46.585225Z",
     "shell.execute_reply": "2025-07-20T09:35:46.584656Z"
    },
    "papermill": {
     "duration": 0.017619,
     "end_time": "2025-07-20T09:35:46.586300",
     "exception": false,
     "start_time": "2025-07-20T09:35:46.568681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Preprocessing functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Enhanced Preprocessing with Logging\n",
    "def extract_frames(video_path, num_frames=None):\n",
    "    \"\"\"Extract frames from video with comprehensive logging\"\"\"\n",
    "    if num_frames is None:\n",
    "        num_frames = CONFIG[\"video_max_frames\"]\n",
    "    \n",
    "    logger.info(f\"Extracting {num_frames} frames from video: {video_path}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(video_path):\n",
    "            raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        duration = total_frames / fps if fps > 0 else 0\n",
    "        \n",
    "        logger.debug(f\"Video info - Total frames: {total_frames}, FPS: {fps:.2f}, Duration: {duration:.2f}s\")\n",
    "        \n",
    "        if total_frames == 0:\n",
    "            raise ValueError(\"Video contains no frames\")\n",
    "        \n",
    "        # Calculate frame indices\n",
    "        if total_frames <= num_frames:\n",
    "            idxs = list(range(total_frames))\n",
    "        else:\n",
    "            idxs = np.linspace(0, total_frames-1, num_frames, dtype=int)\n",
    "        \n",
    "        frames = []\n",
    "        frames_extracted = 0\n",
    "        \n",
    "        for i in idxs:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                logger.warning(f\"Failed to read frame at index {i}\")\n",
    "                continue\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(frame))\n",
    "            frames_extracted += 1\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        logger.info(f\"Frame extraction completed: {frames_extracted}/{len(idxs)} frames in {processing_time:.2f}s\")\n",
    "        \n",
    "        if not frames:\n",
    "            raise ValueError(\"No frames could be extracted from video\")\n",
    "        \n",
    "        return frames\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Frame extraction failed: {str(e)}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "def audio_to_mel(audio_path):\n",
    "    \"\"\"Convert audio to mel-spectrogram with comprehensive logging\"\"\"\n",
    "    logger.info(f\"Converting audio to mel-spectrogram: {audio_path}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "        \n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=CONFIG[\"audio_sr\"], \n",
    "                           duration=CONFIG[\"audio_duration\"])\n",
    "        logger.debug(f\"Loaded audio: shape={y.shape}, sr={sr}\")\n",
    "        \n",
    "        # Ensure consistent length\n",
    "        target_length = int(sr * CONFIG[\"audio_duration\"])\n",
    "        if len(y) > target_length:\n",
    "            y = y[:target_length]\n",
    "            logger.debug(f\"Truncated audio to {target_length} samples\")\n",
    "        else:\n",
    "            y = np.pad(y, (0, target_length - len(y)), 'constant')\n",
    "            logger.debug(f\"Padded audio to {target_length} samples\")\n",
    "        \n",
    "        # Generate mel-spectrogram\n",
    "        S = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr, n_mels=CONFIG[\"audio_n_mels\"],\n",
    "            n_fft=2048, hop_length=512, power=2.0\n",
    "        )\n",
    "        S_db = librosa.power_to_db(S, ref=np.max)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        logger.info(f\"Mel-spectrogram generated: shape={S_db.shape} in {processing_time:.3f}s\")\n",
    "        \n",
    "        return S_db\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Audio preprocessing failed: {str(e)}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "def validate_input_file(file_path, expected_extensions):\n",
    "    \"\"\"Validate input file with logging\"\"\"\n",
    "    logger.debug(f\"Validating input file: {file_path}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {file_path}\")\n",
    "    \n",
    "    file_ext = Path(file_path).suffix.lower()\n",
    "    if file_ext not in expected_extensions:\n",
    "        raise ValueError(f\"Unsupported file type: {file_ext}. Expected: {expected_extensions}\")\n",
    "    \n",
    "    file_size = os.path.getsize(file_path)\n",
    "    logger.info(f\"Input file validated: {file_path} ({file_size} bytes, {file_ext})\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "logger.info(\"Preprocessing functions defined successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea221cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T09:35:46.592607Z",
     "iopub.status.busy": "2025-07-20T09:35:46.592232Z",
     "iopub.status.idle": "2025-07-20T09:35:46.608692Z",
     "shell.execute_reply": "2025-07-20T09:35:46.608188Z"
    },
    "papermill": {
     "duration": 0.021019,
     "end_time": "2025-07-20T09:35:46.609824",
     "exception": false,
     "start_time": "2025-07-20T09:35:46.588805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Inference functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Enhanced Inference Functions with Logging\n",
    "def load_model(model_path, model_type):\n",
    "    logger.info(f\"Loading {model_type} model from: {model_path}\")\n",
    "    start_time = time.time()\n",
    "    checkpoint = torch.load(model_path, map_location=CONFIG[\"device\"])\n",
    "    \n",
    "    # Initialize the correct model\n",
    "    if model_type == \"image\":\n",
    "        model = ImageModel()\n",
    "    elif model_type == \"video\":\n",
    "        model = VideoModel()\n",
    "    elif model_type == \"audio\":\n",
    "        model = AudioModel()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    \n",
    "    # Load state dict with non-strict mode\n",
    "    state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    model.to(CONFIG[\"device\"]).eval()\n",
    "    load_time = time.time() - start_time\n",
    "    logger.info(f\"Loaded {model_type} model in {load_time:.2f}s (strict=False)\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def infer_image(model, image_path):\n",
    "    \"\"\"Image inference with comprehensive logging\"\"\"\n",
    "    logger.info(f\"Starting image inference: {image_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Validate input\n",
    "        validate_input_file(image_path, ['.jpg', '.jpeg', '.png', '.bmp', '.tiff'])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        logger.debug(f\"Image loaded: {img.size}\")\n",
    "        \n",
    "        x = IMAGE_TRANSFORM(img).unsqueeze(0).to(CONFIG[\"device\"])\n",
    "        logger.debug(f\"Image preprocessed: {x.shape}\")\n",
    "        \n",
    "        # Run inference\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            probs = torch.softmax(logits, 1).cpu().numpy()[0]\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            'file_path': image_path,\n",
    "            'modality': 'image',\n",
    "            'probabilities': {\n",
    "                'real': float(probs[0]),\n",
    "                'fake': float(probs[1])\n",
    "            },\n",
    "            'prediction': 'fake' if probs[1] > probs[0] else 'real',\n",
    "            'confidence': float(max(probs)),\n",
    "            'processing_time': processing_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Image inference completed: {result['prediction']} \"\n",
    "                   f\"({result['confidence']:.3f} confidence) in {processing_time:.3f}s\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Image inference failed: {str(e)}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "        return {\n",
    "            'file_path': image_path,\n",
    "            'modality': 'image',\n",
    "            'error': str(e),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "def infer_video(model, video_path):\n",
    "    \"\"\"Video inference with comprehensive logging\"\"\"\n",
    "    logger.info(f\"Starting video inference: {video_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Validate input\n",
    "        validate_input_file(video_path, ['.mp4', '.avi', '.mov', '.mkv', '.webm'])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Extract frames\n",
    "        frames = extract_frames(video_path)\n",
    "        if not frames:\n",
    "            raise ValueError(\"No frames extracted from video\")\n",
    "        \n",
    "        logger.debug(f\"Processing {len(frames)} frames\")\n",
    "        \n",
    "        # Preprocess frames\n",
    "        batch = torch.stack([VIDEO_TRANSFORM(f) for f in frames]).unsqueeze(0).to(CONFIG[\"device\"])\n",
    "        logger.debug(f\"Video batch shape: {batch.shape}\")\n",
    "        \n",
    "        # Run inference\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch)\n",
    "            probs = torch.softmax(logits, 1).cpu().numpy()[0]\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            'file_path': video_path,\n",
    "            'modality': 'video',\n",
    "            'frames_processed': len(frames),\n",
    "            'probabilities': {\n",
    "                'real': float(probs[0]),\n",
    "                'fake': float(probs[1])\n",
    "            },\n",
    "            'prediction': 'fake' if probs[1] > probs[0] else 'real',\n",
    "            'confidence': float(max(probs)),\n",
    "            'processing_time': processing_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Video inference completed: {result['prediction']} \"\n",
    "                   f\"({result['confidence']:.3f} confidence) in {processing_time:.2f}s\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Video inference failed: {str(e)}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "        return {\n",
    "            'file_path': video_path,\n",
    "            'modality': 'video',\n",
    "            'error': str(e),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "def infer_audio(model, audio_path):\n",
    "    \"\"\"Audio inference with comprehensive logging\"\"\"\n",
    "    logger.info(f\"Starting audio inference: {audio_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Validate input\n",
    "        validate_input_file(audio_path, ['.wav', '.mp3', '.flac', '.m4a', '.ogg'])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert to mel-spectrogram\n",
    "        mel = audio_to_mel(audio_path)\n",
    "        x = torch.tensor(mel).unsqueeze(0).unsqueeze(0).float().to(CONFIG[\"device\"])\n",
    "        logger.debug(f\"Audio tensor shape: {x.shape}\")\n",
    "        \n",
    "        # Run inference\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            probs = torch.softmax(logits, 1).cpu().numpy()[0]\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            'file_path': audio_path,\n",
    "            'modality': 'audio',\n",
    "            'probabilities': {\n",
    "                'real': float(probs[0]),\n",
    "                'fake': float(probs[1])\n",
    "            },\n",
    "            'prediction': 'fake' if probs[1] > probs[0] else 'real',\n",
    "            'confidence': float(max(probs)),\n",
    "            'processing_time': processing_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Audio inference completed: {result['prediction']} \"\n",
    "                   f\"({result['confidence']:.3f} confidence) in {processing_time:.3f}s\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Audio inference failed: {str(e)}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "        return {\n",
    "            'file_path': audio_path,\n",
    "            'modality': 'audio',\n",
    "            'error': str(e),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "logger.info(\"Inference functions defined successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc3f94d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T09:35:46.615970Z",
     "iopub.status.busy": "2025-07-20T09:35:46.615792Z",
     "iopub.status.idle": "2025-07-20T09:35:46.645587Z",
     "shell.execute_reply": "2025-07-20T09:35:46.645045Z"
    },
    "papermill": {
     "duration": 0.034077,
     "end_time": "2025-07-20T09:35:46.646634",
     "exception": false,
     "start_time": "2025-07-20T09:35:46.612557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Initializing DeepfakeInferencePipeline with models from: /kaggle/input/all-models-trained\n",
      "INFO - Discovering available models...\n",
      "INFO - Found 5 model files\n",
      "INFO - Available models: ['video', 'image', 'audio']\n",
      "INFO - DeepfakeInferencePipeline initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Unified Inference Interface with Logging\n",
    "class DeepfakeInferencePipeline:\n",
    "    \"\"\"Unified inference pipeline with comprehensive logging\"\"\"\n",
    "    \n",
    "    def __init__(self, models_dir=None):\n",
    "        self.models_dir = models_dir or CONFIG[\"models_dir\"]\n",
    "        self.loaded_models = {}\n",
    "        logger.info(f\"Initializing DeepfakeInferencePipeline with models from: {self.models_dir}\")\n",
    "        \n",
    "        # Discover available models\n",
    "        self.discover_models()\n",
    "    \n",
    "    def discover_models(self):\n",
    "        \"\"\"Discover available model files\"\"\"\n",
    "        logger.info(\"Discovering available models...\")\n",
    "        \n",
    "        if not os.path.exists(self.models_dir):\n",
    "            logger.error(f\"Models directory not found: {self.models_dir}\")\n",
    "            return\n",
    "        \n",
    "        model_files = [f for f in os.listdir(self.models_dir) if f.endswith('.pth')]\n",
    "        logger.info(f\"Found {len(model_files)} model files\")\n",
    "        \n",
    "        self.available_models = {}\n",
    "        for model_file in model_files:\n",
    "            # Parse model type from filename\n",
    "            if 'image' in model_file.lower():\n",
    "                model_type = 'image'\n",
    "            elif 'video' in model_file.lower():\n",
    "                model_type = 'video'\n",
    "            elif 'audio' in model_file.lower():\n",
    "                model_type = 'audio'\n",
    "            else:\n",
    "                logger.warning(f\"Could not determine type for model: {model_file}\")\n",
    "                continue\n",
    "            \n",
    "            model_path = os.path.join(self.models_dir, model_file)\n",
    "            self.available_models[model_type] = {\n",
    "                'path': model_path,\n",
    "                'filename': model_file,\n",
    "                'size': os.path.getsize(model_path)\n",
    "            }\n",
    "            logger.debug(f\"Registered {model_type} model: {model_file}\")\n",
    "        \n",
    "        logger.info(f\"Available models: {list(self.available_models.keys())}\")\n",
    "    \n",
    "    def get_model(self, model_type):\n",
    "        \"\"\"Load and cache model\"\"\"\n",
    "        if model_type not in self.available_models:\n",
    "            raise ValueError(f\"No {model_type} model available. Available: {list(self.available_models.keys())}\")\n",
    "        \n",
    "        if model_type not in self.loaded_models:\n",
    "            model_info = self.available_models[model_type]\n",
    "            logger.info(f\"Loading {model_type} model for first time...\")\n",
    "            self.loaded_models[model_type] = load_model(model_info['path'], model_type)\n",
    "        else:\n",
    "            logger.debug(f\"Using cached {model_type} model\")\n",
    "        \n",
    "        return self.loaded_models[model_type]\n",
    "    \n",
    "    def infer_single(self, file_path, model_type=None):\n",
    "        \"\"\"Run inference on single file\"\"\"\n",
    "        logger.info(f\"Single file inference request: {file_path}\")\n",
    "        \n",
    "        # Auto-detect model type if not specified\n",
    "        if model_type is None:\n",
    "            model_type = self.detect_modality(file_path)\n",
    "            logger.info(f\"Auto-detected modality: {model_type}\")\n",
    "        \n",
    "        # Get appropriate model\n",
    "        model = self.get_model(model_type)\n",
    "        \n",
    "        # Run inference\n",
    "        if model_type == 'image':\n",
    "            return infer_image(model, file_path)\n",
    "        elif model_type == 'video':\n",
    "            return infer_video(model, file_path)\n",
    "        elif model_type == 'audio':\n",
    "            return infer_audio(model, file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported modality: {model_type}\")\n",
    "    \n",
    "    def infer_batch(self, file_paths, model_type=None, save_results=True):\n",
    "        \"\"\"Run inference on multiple files\"\"\"\n",
    "        logger.info(f\"Batch inference request: {len(file_paths)} files\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = []\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        \n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            logger.info(f\"Processing file {i+1}/{len(file_paths)}: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "                result = self.infer_single(file_path, model_type)\n",
    "                if 'error' not in result:\n",
    "                    successful += 1\n",
    "                else:\n",
    "                    failed += 1\n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Batch inference failed for {file_path}: {str(e)}\")\n",
    "                failed += 1\n",
    "                results.append({\n",
    "                    'file_path': file_path,\n",
    "                    'error': str(e),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        batch_summary = {\n",
    "            'total_files': len(file_paths),\n",
    "            'successful': successful,\n",
    "            'failed': failed,\n",
    "            'total_time': total_time,\n",
    "            'avg_time_per_file': total_time / len(file_paths),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Batch inference completed: {successful}/{len(file_paths)} successful \"\n",
    "                   f\"in {total_time:.2f}s (avg: {batch_summary['avg_time_per_file']:.3f}s/file)\")\n",
    "        \n",
    "        # Save results if requested\n",
    "        if save_results:\n",
    "            self.save_results(batch_summary)\n",
    "        \n",
    "        return batch_summary\n",
    "    \n",
    "    def detect_modality(self, file_path):\n",
    "        \"\"\"Auto-detect file modality based on extension\"\"\"\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        \n",
    "        if ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
    "            return 'image'\n",
    "        elif ext in ['.mp4', '.avi', '.mov', '.mkv', '.webm']:\n",
    "            return 'video'\n",
    "        elif ext in ['.wav', '.mp3', '.flac', '.m4a', '.ogg']:\n",
    "            return 'audio'\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot auto-detect modality for file extension: {ext}\")\n",
    "    \n",
    "    def save_results(self, results, filename=None):\n",
    "        \"\"\"Save results to JSON file\"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"/kaggle/working/inference_results_{timestamp}.json\"\n",
    "        \n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(results, f, indent=2, default=str)\n",
    "            logger.info(f\"Results saved to: {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save results: {str(e)}\")\n",
    "    \n",
    "    def get_system_info(self):\n",
    "        \"\"\"Get system information for logging\"\"\"\n",
    "        return {\n",
    "            'device': CONFIG['device'],\n",
    "            'available_models': list(self.available_models.keys()),\n",
    "            'loaded_models': list(self.loaded_models.keys()),\n",
    "            'torch_version': torch.__version__,\n",
    "            'cuda_available': torch.cuda.is_available(),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Initialize global pipeline instance\n",
    "pipeline = DeepfakeInferencePipeline()\n",
    "logger.info(\"DeepfakeInferencePipeline initialized successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beef8b1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T09:35:46.653722Z",
     "iopub.status.busy": "2025-07-20T09:35:46.653537Z",
     "iopub.status.idle": "2025-07-20T09:36:19.494185Z",
     "shell.execute_reply": "2025-07-20T09:36:19.493369Z"
    },
    "papermill": {
     "duration": 32.845565,
     "end_time": "2025-07-20T09:36:19.495430",
     "exception": false,
     "start_time": "2025-07-20T09:35:46.649865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - ==================================================\n",
      "INFO - SYSTEM INFORMATION\n",
      "INFO - ==================================================\n",
      "INFO - device: cuda\n",
      "INFO - available_models: ['video', 'image', 'audio']\n",
      "INFO - loaded_models: []\n",
      "INFO - torch_version: 2.6.0+cu124\n",
      "INFO - cuda_available: True\n",
      "INFO - timestamp: 2025-07-20T09:35:46.666091\n",
      "INFO - ==================================================\n",
      "INFO - TESTING SINGLE FILE INFERENCE\n",
      "INFO - ==================================================\n",
      "INFO - Testing image inference...\n",
      "INFO - Single file inference request: /kaggle/input/sample-media/IMG-20250713-WA0027.jpg\n",
      "INFO - Auto-detected modality: image\n",
      "INFO - Loading image model for first time...\n",
      "INFO - Loading image model from: /kaggle/input/all-models-trained/celebdf_v2_image_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.13.1\" to /root/.cache/torch/hub/v0.13.1.zip\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Loaded image model in 3.34s (strict=False)\n",
      "INFO - Starting image inference: /kaggle/input/sample-media/IMG-20250713-WA0027.jpg\n",
      "INFO - Input file validated: /kaggle/input/sample-media/IMG-20250713-WA0027.jpg (110722 bytes, .jpg)\n",
      "INFO - Image inference completed: fake (0.542 confidence) in 1.728s\n",
      "INFO - Result: {\n",
      "  \"file_path\": \"/kaggle/input/sample-media/IMG-20250713-WA0027.jpg\",\n",
      "  \"modality\": \"image\",\n",
      "  \"probabilities\": {\n",
      "    \"real\": 0.45793306827545166,\n",
      "    \"fake\": 0.5420669317245483\n",
      "  },\n",
      "  \"prediction\": \"fake\",\n",
      "  \"confidence\": 0.5420669317245483,\n",
      "  \"processing_time\": 1.727529764175415,\n",
      "  \"timestamp\": \"2025-07-20T09:35:51.751952\"\n",
      "}\n",
      "INFO - Testing video inference...\n",
      "INFO - Single file inference request: /kaggle/input/sample-media/Ayush - Slide 6.mp4\n",
      "INFO - Auto-detected modality: video\n",
      "INFO - Loading video model for first time...\n",
      "INFO - Loading video model from: /kaggle/input/all-models-trained/ff_c23_video_best.pth\n",
      "INFO - Loaded video model in 1.28s (strict=False)\n",
      "INFO - Starting video inference: /kaggle/input/sample-media/Ayush - Slide 6.mp4\n",
      "INFO - Input file validated: /kaggle/input/sample-media/Ayush - Slide 6.mp4 (2137189 bytes, .mp4)\n",
      "INFO - Extracting 8 frames from video: /kaggle/input/sample-media/Ayush - Slide 6.mp4\n",
      "INFO - Frame extraction completed: 8/8 frames in 4.09s\n",
      "INFO - Video inference completed: real (0.521 confidence) in 4.40s\n",
      "INFO - Result: {\n",
      "  \"file_path\": \"/kaggle/input/sample-media/Ayush - Slide 6.mp4\",\n",
      "  \"modality\": \"video\",\n",
      "  \"frames_processed\": 8,\n",
      "  \"probabilities\": {\n",
      "    \"real\": 0.5212072730064392,\n",
      "    \"fake\": 0.4787926971912384\n",
      "  },\n",
      "  \"prediction\": \"real\",\n",
      "  \"confidence\": 0.5212072730064392,\n",
      "  \"processing_time\": 4.402581453323364,\n",
      "  \"timestamp\": \"2025-07-20T09:35:57.454003\"\n",
      "}\n",
      "INFO - Testing audio inference...\n",
      "INFO - Single file inference request: /kaggle/input/sample-media/file_example_WAV_2MG.wav\n",
      "INFO - Auto-detected modality: audio\n",
      "INFO - Loading audio model for first time...\n",
      "INFO - Loading audio model from: /kaggle/input/all-models-trained/in_the_wild_audio_audio_best.pth\n",
      "INFO - Loaded audio model in 0.05s (strict=False)\n",
      "INFO - Starting audio inference: /kaggle/input/sample-media/file_example_WAV_2MG.wav\n",
      "INFO - Input file validated: /kaggle/input/sample-media/file_example_WAV_2MG.wav (2104474 bytes, .wav)\n",
      "INFO - Converting audio to mel-spectrogram: /kaggle/input/sample-media/file_example_WAV_2MG.wav\n",
      "INFO - Mel-spectrogram generated: shape=(128, 79) in 18.214s\n",
      "INFO - Audio inference completed: real (0.753 confidence) in 18.239s\n",
      "INFO - Result: {\n",
      "  \"file_path\": \"/kaggle/input/sample-media/file_example_WAV_2MG.wav\",\n",
      "  \"modality\": \"audio\",\n",
      "  \"probabilities\": {\n",
      "    \"real\": 0.753468930721283,\n",
      "    \"fake\": 0.24653103947639465\n",
      "  },\n",
      "  \"prediction\": \"real\",\n",
      "  \"confidence\": 0.753468930721283,\n",
      "  \"processing_time\": 18.239235401153564,\n",
      "  \"timestamp\": \"2025-07-20T09:36:15.766871\"\n",
      "}\n",
      "INFO - ==================================================\n",
      "INFO - TESTING BATCH INFERENCE\n",
      "INFO - ==================================================\n",
      "INFO - Batch inference request: 2 files\n",
      "INFO - Processing file 1/2: /kaggle/input/sample-media/IMG-20250713-WA0027.jpg\n",
      "INFO - Single file inference request: /kaggle/input/sample-media/IMG-20250713-WA0027.jpg\n",
      "INFO - Auto-detected modality: image\n",
      "INFO - Starting image inference: /kaggle/input/sample-media/IMG-20250713-WA0027.jpg\n",
      "INFO - Input file validated: /kaggle/input/sample-media/IMG-20250713-WA0027.jpg (110722 bytes, .jpg)\n",
      "INFO - Image inference completed: fake (0.542 confidence) in 0.038s\n",
      "INFO - Processing file 2/2: /kaggle/input/sample-media/Ayush - Slide 6.mp4\n",
      "INFO - Single file inference request: /kaggle/input/sample-media/Ayush - Slide 6.mp4\n",
      "INFO - Auto-detected modality: video\n",
      "INFO - Starting video inference: /kaggle/input/sample-media/Ayush - Slide 6.mp4\n",
      "INFO - Input file validated: /kaggle/input/sample-media/Ayush - Slide 6.mp4 (2137189 bytes, .mp4)\n",
      "INFO - Extracting 8 frames from video: /kaggle/input/sample-media/Ayush - Slide 6.mp4\n",
      "INFO - Frame extraction completed: 8/8 frames in 3.54s\n",
      "INFO - Video inference completed: real (0.521 confidence) in 3.65s\n",
      "INFO - Batch inference completed: 2/2 successful in 3.71s (avg: 1.854s/file)\n",
      "INFO - Results saved to: /kaggle/working/inference_results_20250720_093619.json\n",
      "INFO - Batch results summary: 2/2 successful\n",
      "INFO - ==================================================\n",
      "INFO - COMMAND-LINE INTERFACE\n",
      "INFO - ==================================================\n",
      "INFO - Notebook environment - CLI skipped\n",
      "INFO - ==================================================\n",
      "INFO - INFERENCE PIPELINE READY\n",
      "INFO - ==================================================\n",
      "INFO - Available models: ['video', 'image', 'audio']\n",
      "INFO - Log file: /kaggle/working/logs/deepfake_inference_20250720_093546.log\n",
      "INFO - Use pipeline.infer_single() or pipeline.infer_batch() for inference\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Example Usage with Comprehensive Logging\n",
    "\n",
    "# Example 1: Single file inference\n",
    "def test_single_inference():\n",
    "    \"\"\"Test single file inference with logging\"\"\"\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"TESTING SINGLE FILE INFERENCE\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    # Example paths (replace with actual test files)\n",
    "    test_files = {\n",
    "         'image': '/kaggle/input/sample-media/IMG-20250713-WA0027.jpg',\n",
    "         'video': '/kaggle/input/sample-media/Ayush - Slide 6.mp4',\n",
    "         'audio': '/kaggle/input/sample-media/file_example_WAV_2MG.wav'\n",
    "    }\n",
    "    \n",
    "    for modality, file_path in test_files.items():\n",
    "        if os.path.exists(file_path):\n",
    "            logger.info(f\"Testing {modality} inference...\")\n",
    "            try:\n",
    "                result = pipeline.infer_single(file_path)\n",
    "                logger.info(f\"Result: {json.dumps(result, indent=2)}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Test failed for {modality}: {str(e)}\")\n",
    "        else:\n",
    "            logger.warning(f\"Test file not found: {file_path}\")\n",
    "\n",
    "# Example 2: Batch inference\n",
    "def test_batch_inference():\n",
    "    \"\"\"Test batch inference with logging\"\"\"\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"TESTING BATCH INFERENCE\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    # Example batch (replace with actual file paths)\n",
    "    test_batch = [\n",
    "         '/kaggle/input/sample-media/IMG-20250713-WA0027.jpg',\n",
    "         '/kaggle/input/sample-media/Ayush - Slide 6.mp4'\n",
    "    ]\n",
    "    \n",
    "    if test_batch:\n",
    "        try:\n",
    "            results = pipeline.infer_batch(test_batch, save_results=True)\n",
    "            logger.info(f\"Batch results summary: {results['successful']}/{results['total_files']} successful\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Batch test failed: {str(e)}\")\n",
    "    else:\n",
    "        logger.info(\"No test files specified for batch inference\")\n",
    "\n",
    "# Example 3: Command-line interface\n",
    "def run_cli():\n",
    "    \"\"\"Command-line interface with proper logging\"\"\"\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"COMMAND-LINE INTERFACE\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    # Only run CLI in script mode, not in Jupyter\n",
    "    if __name__ == \"__main__\" and not hasattr(sys, \"ps1\"):\n",
    "        try:\n",
    "            parser = argparse.ArgumentParser(\n",
    "                description=\"Deepfake Detection Inference Pipeline\",\n",
    "                formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "                epilog=\"\"\"\n",
    "Examples:\n",
    "  python inference.py single image /path/to/image.jpg\n",
    "  python inference.py single auto /path/to/file.mp4\n",
    "  python inference.py batch /path/to/folder/\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            parser.add_argument(\"mode\", choices=[\"single\", \"batch\"], \n",
    "                              help=\"Inference mode\")\n",
    "            parser.add_argument(\"modality\", nargs='?', \n",
    "                              choices=[\"image\", \"video\", \"audio\", \"auto\"], \n",
    "                              default=\"auto\",\n",
    "                              help=\"Input modality (auto-detect if not specified)\")\n",
    "            parser.add_argument(\"input\", help=\"Input file or folder path\")\n",
    "            parser.add_argument(\"--output\", help=\"Output file path for results\")\n",
    "            parser.add_argument(\"--log-level\", default=\"INFO\",\n",
    "                              choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"],\n",
    "                              help=\"Logging level\")\n",
    "            \n",
    "            args, extra = parser.parse_known_args()\n",
    "            \n",
    "            # Update logging level\n",
    "            if args.log_level != \"INFO\":\n",
    "                logging.getLogger().setLevel(getattr(logging, args.log_level))\n",
    "                logger.info(f\"Log level changed to: {args.log_level}\")\n",
    "            \n",
    "            logger.info(f\"CLI Arguments: {vars(args)}\")\n",
    "            \n",
    "            if args.mode == \"single\":\n",
    "                result = pipeline.infer_single(args.input, \n",
    "                                             None if args.modality == \"auto\" else args.modality)\n",
    "                print(json.dumps(result, indent=2))\n",
    "                \n",
    "                if args.output:\n",
    "                    pipeline.save_results(result, args.output)\n",
    "                    \n",
    "            elif args.mode == \"batch\":\n",
    "                # Collect files from directory\n",
    "                if os.path.isdir(args.input):\n",
    "                    files = []\n",
    "                    for ext in ['jpg', 'jpeg', 'png', 'mp4', 'avi', 'wav', 'mp3']:\n",
    "                        files.extend(Path(args.input).glob(f\"*.{ext}\"))\n",
    "                    file_paths = [str(f) for f in files]\n",
    "                else:\n",
    "                    file_paths = [args.input]\n",
    "                \n",
    "                logger.info(f\"Found {len(file_paths)} files for batch processing\")\n",
    "                \n",
    "                results = pipeline.infer_batch(file_paths,\n",
    "                                             None if args.modality == \"auto\" else args.modality,\n",
    "                                             save_results=True)\n",
    "                \n",
    "                print(f\"Batch completed: {results['successful']}/{results['total_files']} successful\")\n",
    "                \n",
    "                if args.output:\n",
    "                    pipeline.save_results(results, args.output)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"CLI execution failed: {str(e)}\")\n",
    "            logger.debug(traceback.format_exc())\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        logger.info(\"Notebook environment - CLI skipped\")\n",
    "\n",
    "# System information logging\n",
    "logger.info(\"=\"*50)\n",
    "logger.info(\"SYSTEM INFORMATION\")\n",
    "logger.info(\"=\"*50)\n",
    "system_info = pipeline.get_system_info()\n",
    "for key, value in system_info.items():\n",
    "    logger.info(f\"{key}: {value}\")\n",
    "\n",
    "# Run tests (uncomment to execute)\n",
    "test_single_inference()\n",
    "test_batch_inference()\n",
    "run_cli()\n",
    "\n",
    "logger.info(\"=\"*50)\n",
    "logger.info(\"INFERENCE PIPELINE READY\")\n",
    "logger.info(\"=\"*50)\n",
    "logger.info(f\"Available models: {list(pipeline.available_models.keys())}\")\n",
    "logger.info(f\"Log file: {LOG_FILE}\")\n",
    "logger.info(\"Use pipeline.infer_single() or pipeline.infer_batch() for inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caf0f1c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T09:36:19.508550Z",
     "iopub.status.busy": "2025-07-20T09:36:19.508329Z",
     "iopub.status.idle": "2025-07-20T09:36:19.514283Z",
     "shell.execute_reply": "2025-07-20T09:36:19.513232Z"
    },
    "papermill": {
     "duration": 0.014225,
     "end_time": "2025-07-20T09:36:19.515740",
     "exception": false,
     "start_time": "2025-07-20T09:36:19.501515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================================================\n",
      "DEEPFAKE DETECTION INFERENCE PIPELINE - USAGE DOCUMENTATION\n",
      "=============================================================================\n",
      "\n",
      "FEATURES:\n",
      "- Comprehensive logging to file and console\n",
      "- Support for image, video, and audio inference\n",
      "- Automatic model loading and caching\n",
      "- Batch processing capabilities\n",
      "- Error handling and recovery\n",
      "- Detailed performance metrics\n",
      "- JSON output format\n",
      "\n",
      "USAGE EXAMPLES:\n",
      "\n",
      "1. Single File Inference:\n",
      "   result = pipeline.infer_single('/path/to/image.jpg')\n",
      "   result = pipeline.infer_single('/path/to/video.mp4', 'video')\n",
      "   \n",
      "2. Batch Inference:\n",
      "   files = ['/path/to/file1.jpg', '/path/to/file2.mp4']\n",
      "   results = pipeline.infer_batch(files, save_results=True)\n",
      "   \n",
      "3. Auto-detect Modality:\n",
      "   result = pipeline.infer_single('/path/to/unknown_file.ext')\n",
      "   \n",
      "4. Save Results:\n",
      "   pipeline.save_results(result, '/path/to/output.json')\n",
      "\n",
      "OUTPUT FORMAT:\n",
      "{\n",
      "  \"file_path\": \"/path/to/file.jpg\",\n",
      "  \"modality\": \"image\",\n",
      "  \"probabilities\": {\n",
      "    \"real\": 0.234,\n",
      "    \"fake\": 0.766\n",
      "  },\n",
      "  \"prediction\": \"fake\",\n",
      "  \"confidence\": 0.766,\n",
      "  \"processing_time\": 0.123,\n",
      "  \"timestamp\": \"2025-07-20T13:54:00\"\n",
      "}\n",
      "\n",
      "LOG FILES:\n",
      "- Main log: /kaggle/working/logs/deepfake_inference_YYYYMMDD_HHMMSS.log\n",
      "- Results: /kaggle/working/inference_results_YYYYMMDD_HHMMSS.json\n",
      "\n",
      "AVAILABLE MODELS:\n",
      "\n",
      "- VIDEO: ff_c23_video_best.pth (70.8 MB)\n",
      "- IMAGE: celebdf_v2_image_best.pth (53.9 MB)\n",
      "- AUDIO: in_the_wild_audio_audio_best.pth (1.2 MB)\n",
      "\n",
      "SYSTEM STATUS:\n",
      "- Device: cuda\n",
      "- Models Directory: /kaggle/input/all-models-trained\n",
      "- Log Level: 10\n",
      "- PyTorch Version: 2.6.0+cu124\n",
      "\n",
      "Ready for inference! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Usage Examples and Documentation\n",
    "\n",
    "print(\"\"\"\n",
    "=============================================================================\n",
    "DEEPFAKE DETECTION INFERENCE PIPELINE - USAGE DOCUMENTATION\n",
    "=============================================================================\n",
    "\n",
    "FEATURES:\n",
    "- Comprehensive logging to file and console\n",
    "- Support for image, video, and audio inference\n",
    "- Automatic model loading and caching\n",
    "- Batch processing capabilities\n",
    "- Error handling and recovery\n",
    "- Detailed performance metrics\n",
    "- JSON output format\n",
    "\n",
    "USAGE EXAMPLES:\n",
    "\n",
    "1. Single File Inference:\n",
    "   result = pipeline.infer_single('/path/to/image.jpg')\n",
    "   result = pipeline.infer_single('/path/to/video.mp4', 'video')\n",
    "   \n",
    "2. Batch Inference:\n",
    "   files = ['/path/to/file1.jpg', '/path/to/file2.mp4']\n",
    "   results = pipeline.infer_batch(files, save_results=True)\n",
    "   \n",
    "3. Auto-detect Modality:\n",
    "   result = pipeline.infer_single('/path/to/unknown_file.ext')\n",
    "   \n",
    "4. Save Results:\n",
    "   pipeline.save_results(result, '/path/to/output.json')\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "{\n",
    "  \"file_path\": \"/path/to/file.jpg\",\n",
    "  \"modality\": \"image\",\n",
    "  \"probabilities\": {\n",
    "    \"real\": 0.234,\n",
    "    \"fake\": 0.766\n",
    "  },\n",
    "  \"prediction\": \"fake\",\n",
    "  \"confidence\": 0.766,\n",
    "  \"processing_time\": 0.123,\n",
    "  \"timestamp\": \"2025-07-20T13:54:00\"\n",
    "}\n",
    "\n",
    "LOG FILES:\n",
    "- Main log: /kaggle/working/logs/deepfake_inference_YYYYMMDD_HHMMSS.log\n",
    "- Results: /kaggle/working/inference_results_YYYYMMDD_HHMMSS.json\n",
    "\n",
    "AVAILABLE MODELS:\n",
    "\"\"\")\n",
    "\n",
    "for model_type, info in pipeline.available_models.items():\n",
    "    print(f\"- {model_type.upper()}: {info['filename']} ({info['size']/1024/1024:.1f} MB)\")\n",
    "\n",
    "print(f\"\"\"\n",
    "SYSTEM STATUS:\n",
    "- Device: {CONFIG['device']}\n",
    "- Models Directory: {CONFIG['models_dir']}\n",
    "- Log Level: {logging.getLogger().level}\n",
    "- PyTorch Version: {torch.__version__}\n",
    "\n",
    "Ready for inference! \n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7904395,
     "sourceId": 12522303,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7904569,
     "sourceId": 12522566,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 58.326027,
   "end_time": "2025-07-20T09:36:23.028885",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-20T09:35:24.702858",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
