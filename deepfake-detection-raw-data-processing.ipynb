{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4bd35d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T15:58:41.645803Z",
     "iopub.status.busy": "2025-07-15T15:58:41.645225Z",
     "iopub.status.idle": "2025-07-15T15:58:54.462762Z",
     "shell.execute_reply": "2025-07-15T15:58:54.462165Z"
    },
    "papermill": {
     "duration": 12.822472,
     "end_time": "2025-07-15T15:58:54.464099",
     "exception": false,
     "start_time": "2025-07-15T15:58:41.641627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81dfe17e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T15:58:54.472494Z",
     "iopub.status.busy": "2025-07-15T15:58:54.472145Z",
     "iopub.status.idle": "2025-07-15T15:58:54.476118Z",
     "shell.execute_reply": "2025-07-15T15:58:54.475545Z"
    },
    "papermill": {
     "duration": 0.00822,
     "end_time": "2025-07-15T15:58:54.477271",
     "exception": false,
     "start_time": "2025-07-15T15:58:54.469051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "KAGGLE_INPUT_PATH = \"/kaggle/input\"\n",
    "PROCESSED_DATA_PATH = \"/kaggle/working/processed_data\"\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "408c2f14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T15:58:54.482020Z",
     "iopub.status.busy": "2025-07-15T15:58:54.481824Z",
     "iopub.status.idle": "2025-07-15T15:58:54.505504Z",
     "shell.execute_reply": "2025-07-15T15:58:54.504773Z"
    },
    "papermill": {
     "duration": 0.027479,
     "end_time": "2025-07-15T15:58:54.506554",
     "exception": false,
     "start_time": "2025-07-15T15:58:54.479075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class RobustMultiModalDatasetManager:\n",
    "    def __init__(self, kaggle_input_path=\"/kaggle/input\"):\n",
    "        self.kaggle_input_path = kaggle_input_path\n",
    "        self.processed_data_path = \"/kaggle/working/processed_data\"\n",
    "        os.makedirs(self.processed_data_path, exist_ok=True)\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def extract_frames_from_video(self, video_path, num_frames=5, target_size=(224, 224)):\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                return []\n",
    "            frames = []\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            if frame_count == 0:\n",
    "                cap.release()\n",
    "                return []\n",
    "            frame_indices = np.linspace(0, frame_count-1, num_frames, dtype=int) if frame_count > num_frames else list(range(frame_count))\n",
    "            for frame_idx in frame_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frame = cv2.resize(frame, target_size)\n",
    "                    frames.append(frame)\n",
    "            cap.release()\n",
    "            return frames\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def process_ff_c23_dataset(self):\n",
    "        print(\"Processing FF-C23 dataset...\")\n",
    "        dataset_path = os.path.join(self.kaggle_input_path, \"ff-c23\", \"FaceForensics++_C23\")\n",
    "        processed_data = []\n",
    "        categories = ['original', 'Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures']\n",
    "        for category in categories:\n",
    "            category_path = os.path.join(dataset_path, category)\n",
    "            if os.path.exists(category_path):\n",
    "                label = 0 if category == 'original' else 1\n",
    "                for video_file in os.listdir(category_path):\n",
    "                    if video_file.endswith('.mp4'):\n",
    "                        video_path = os.path.join(category_path, video_file)\n",
    "                        frames = self.extract_frames_from_video(video_path)\n",
    "                        if frames:\n",
    "                            processed_data.append({'video_file': video_file, 'category': category, 'label': label, 'frames': frames, 'num_frames': len(frames)})\n",
    "        pd.DataFrame(processed_data).to_pickle(os.path.join(self.processed_data_path, \"ff_c23_processed.pkl\"))\n",
    "        print(f\"FF-C23 processed: {len(processed_data)} videos\")\n",
    "        return processed_data\n",
    "\n",
    "    def process_celebdf_v2_dataset(self):\n",
    "        print(\"Processing CelebDF-v2 dataset...\")\n",
    "        dataset_path = os.path.join(self.kaggle_input_path, \"celebdf-v2image-dataset\", \"Celeb_V2\")\n",
    "        processed_data = []\n",
    "        for split in [\"Train\", \"Test\", \"Val\"]:\n",
    "            for label_folder in [\"real\", \"fake\"]:\n",
    "                folder_path = os.path.join(dataset_path, split, label_folder)\n",
    "                if os.path.exists(folder_path):\n",
    "                    label = 0 if label_folder == \"real\" else 1\n",
    "                    for img_file in os.listdir(folder_path):\n",
    "                        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                            img_path = os.path.join(folder_path, img_file)\n",
    "                            processed_data.append({'image_file': img_file, 'image_path': img_path, 'split': split, 'label': label, 'label_name': label_folder})\n",
    "        pd.DataFrame(processed_data).to_csv(os.path.join(self.processed_data_path, \"celebdf_v2_processed.csv\"), index=False)\n",
    "        print(f\"CelebDF-v2 processed: {len(processed_data)} images\")\n",
    "        return processed_data\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def process_asvspoof_21_cqt_dataset(self):\n",
    "        print(\"Processing ASVspoof-21 CQT dataset...\")\n",
    "        dataset_path = os.path.join(self.kaggle_input_path, \"asvspoof-21-df-cqt\", \"my_dataset\")\n",
    "        processed_data = []\n",
    "        for split in ['train', 'test', 'validation']:\n",
    "            for label_folder in ['real', 'fake']:\n",
    "                folder_path = os.path.join(dataset_path, split, label_folder)\n",
    "                if os.path.exists(folder_path):\n",
    "                    label = 0 if label_folder == \"real\" else 1\n",
    "                    for img_file in os.listdir(folder_path):\n",
    "                        if img_file.lower().endswith('.png'):\n",
    "                            img_path = os.path.join(folder_path, img_file)\n",
    "                            processed_data.append({'image_file': img_file, 'image_path': img_path, 'split': split, 'label': label, 'label_name': label_folder})\n",
    "        pd.DataFrame(processed_data).to_csv(os.path.join(self.processed_data_path, \"asvspoof_21_cqt_processed.csv\"), index=False)\n",
    "        print(f\"ASVspoof-21 CQT processed: {len(processed_data)} images\")\n",
    "        return processed_data\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def process_dfdc_faces_dataset(self):\n",
    "        print(\"Processing DFDC faces dataset...\")\n",
    "        dataset_path = os.path.join(self.kaggle_input_path, \"dfdc-faces-of-the-train-sample\")\n",
    "        processed_data = []\n",
    "        for split in ['train', 'validation']:\n",
    "            for label_folder in ['real', 'fake']:\n",
    "                folder_path = os.path.join(dataset_path, split, label_folder)\n",
    "                if os.path.exists(folder_path):\n",
    "                    label = 0 if label_folder == 'real' else 1\n",
    "                    for img_file in os.listdir(folder_path):\n",
    "                        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                            img_path = os.path.join(folder_path, img_file)\n",
    "                            processed_data.append({'image_file': img_file, 'image_path': img_path, 'split': split, 'label': label, 'label_name': label_folder})\n",
    "        pd.DataFrame(processed_data).to_csv(os.path.join(self.processed_data_path, \"dfdc_faces_processed.csv\"), index=False)\n",
    "        print(f\"DFDC faces processed: {len(processed_data)} images\")\n",
    "        return processed_data\n",
    "\n",
    "    def process_in_the_wild_audio(self):\n",
    "        print(\"Processing In-The-Wild audio deepfake dataset...\")\n",
    "        dataset_path = os.path.join(self.kaggle_input_path, \"in-the-wild-audio-deepfake\", \"release_in_the_wild\")\n",
    "        processed_data = []\n",
    "        for label_folder in [\"real\", \"fake\"]:\n",
    "            folder_path = os.path.join(dataset_path, label_folder)\n",
    "            if os.path.exists(folder_path):\n",
    "                label = 0 if label_folder == \"real\" else 1\n",
    "                for audio_file in os.listdir(folder_path):\n",
    "                    audio_path = os.path.join(folder_path, audio_file)\n",
    "                    processed_data.append({\n",
    "                        'audio_file': audio_file,\n",
    "                        'audio_path': audio_path,\n",
    "                        'label': label,\n",
    "                        'label_name': label_folder\n",
    "                    })\n",
    "        pd.DataFrame(processed_data).to_csv(os.path.join(self.processed_data_path, \"in_the_wild_audio_processed.csv\"), index=False)\n",
    "        print(f\"In-The-Wild audio processed: {len(processed_data)} samples\")\n",
    "        return processed_data\n",
    "\n",
    "    def process_all_datasets(self):\n",
    "        print(\"Starting comprehensive dataset processing...\")\n",
    "        print(\"=\" * 80)\n",
    "        results = {}\n",
    "        datasets = [\n",
    "            ('ff_c23', self.process_ff_c23_dataset),\n",
    "            ('celebdf_v2', self.process_celebdf_v2_dataset),\n",
    "            ('asvspoof_21_cqt', self.process_asvspoof_21_cqt_dataset),\n",
    "            ('dfdc_faces', self.process_dfdc_faces_dataset),\n",
    "            ('in_the_wild_audio', self.process_in_the_wild_audio)\n",
    "        ]\n",
    "        for dataset_name, process_func in datasets:\n",
    "            try:\n",
    "                print(f\"\\n{'-'*60}\")\n",
    "                results[dataset_name] = process_func()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {dataset_name}: {e}\")\n",
    "                results[dataset_name] = []\n",
    "        self.generate_processing_summary(results)\n",
    "        return results\n",
    "\n",
    "    def generate_processing_summary(self, results):\n",
    "        summary = {'Dataset': [], 'Status': [], 'Total_Samples': [], 'Real_Samples': [], 'Fake_Samples': [], 'Data_Type': []}\n",
    "        for dataset_name, data in results.items():\n",
    "            summary['Dataset'].append(dataset_name)\n",
    "            if data and len(data) > 0:\n",
    "                summary['Status'].append('Success')\n",
    "                summary['Total_Samples'].append(len(data))\n",
    "                real_count = sum(1 for item in data if item.get('label') == 0)\n",
    "                fake_count = sum(1 for item in data if item.get('label') == 1)\n",
    "                summary['Real_Samples'].append(real_count)\n",
    "                summary['Fake_Samples'].append(fake_count)\n",
    "                if 'frames' in data[0]:\n",
    "                    summary['Data_Type'].append('Video')\n",
    "                elif 'image_path' in data[0]:\n",
    "                    summary['Data_Type'].append('Image')\n",
    "                elif 'audio_path' in data[0]:\n",
    "                    summary['Data_Type'].append('Audio')\n",
    "                else:\n",
    "                    summary['Data_Type'].append('Unknown')\n",
    "            else:\n",
    "                summary['Status'].append('Failed/Empty')\n",
    "                summary['Total_Samples'].append(0)\n",
    "                summary['Real_Samples'].append(0)\n",
    "                summary['Fake_Samples'].append(0)\n",
    "                summary['Data_Type'].append('Unknown')\n",
    "        summary_df = pd.DataFrame(summary)\n",
    "        summary_path = os.path.join(self.processed_data_path, \"processing_summary.csv\")\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DATASET PROCESSING SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(summary_df.to_string(index=False))\n",
    "        print(f\"\\nSummary saved to: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02478943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T15:58:54.511047Z",
     "iopub.status.busy": "2025-07-15T15:58:54.510839Z",
     "iopub.status.idle": "2025-07-15T15:58:54.527500Z",
     "shell.execute_reply": "2025-07-15T15:58:54.526978Z"
    },
    "papermill": {
     "duration": 0.020232,
     "end_time": "2025-07-15T15:58:54.528570",
     "exception": false,
     "start_time": "2025-07-15T15:58:54.508338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Advanced Data Augmentation Pipeline\n",
    "class AdvancedDataAugmentation:\n",
    "    \"\"\"\n",
    "    Advanced data augmentation techniques for deepfake detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.setup_augmentations()\n",
    "    \n",
    "    def setup_augmentations(self):\n",
    "        \"\"\"Setup various augmentation techniques\"\"\"\n",
    "        \n",
    "        # Geometric augmentations\n",
    "        self.geometric_transforms = transforms.Compose([\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "        ])\n",
    "        \n",
    "        # Color augmentations\n",
    "        self.color_transforms = transforms.Compose([\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, \n",
    "                                 saturation=0.3, hue=0.1),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "        ])\n",
    "        \n",
    "        # Noise augmentations\n",
    "        self.noise_transforms = transforms.Compose([\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "            transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "        ])\n",
    "    \n",
    "    def apply_cutmix(self, images, labels, alpha=1.0):\n",
    "        \"\"\"Apply CutMix augmentation\"\"\"\n",
    "        batch_size = images.size(0)\n",
    "        indices = torch.randperm(batch_size)\n",
    "        \n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        \n",
    "        bbx1, bby1, bbx2, bby2 = self.rand_bbox(images.size(), lam)\n",
    "        images[:, :, bbx1:bbx2, bby1:bby2] = images[indices, :, bbx1:bbx2, bby1:bby2]\n",
    "        \n",
    "        # Adjust labels\n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size()[-1] * images.size()[-2]))\n",
    "        \n",
    "        return images, labels, labels[indices], lam\n",
    "    \n",
    "    def rand_bbox(self, size, lam):\n",
    "        \"\"\"Generate random bounding box for CutMix\"\"\"\n",
    "        W = size[2]\n",
    "        H = size[3]\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w = np.int(W * cut_rat)\n",
    "        cut_h = np.int(H * cut_rat)\n",
    "        \n",
    "        # Uniform\n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "        \n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "        \n",
    "        return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Audio-specific preprocessing\n",
    "class AudioPreprocessor:\n",
    "    \"\"\"\n",
    "    Audio preprocessing utilities for deepfake detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def extract_mfcc_features(self, audio_path, n_mfcc=13):\n",
    "        \"\"\"Extract MFCC features from audio\"\"\"\n",
    "        audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "        return mfcc\n",
    "    \n",
    "    def extract_spectral_features(self, audio_path):\n",
    "        \"\"\"Extract spectral features\"\"\"\n",
    "        audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        \n",
    "        features = {\n",
    "            'spectral_centroid': librosa.feature.spectral_centroid(y=audio, sr=sr),\n",
    "            'spectral_rolloff': librosa.feature.spectral_rolloff(y=audio, sr=sr),\n",
    "            'zero_crossing_rate': librosa.feature.zero_crossing_rate(audio),\n",
    "            'tempo': librosa.beat.tempo(y=audio, sr=sr)[0]\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def apply_audio_augmentation(self, audio, sr):\n",
    "        \"\"\"Apply audio augmentation techniques\"\"\"\n",
    "        # Time stretching\n",
    "        if np.random.random() < 0.5:\n",
    "            rate = np.random.uniform(0.8, 1.2)\n",
    "            audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "        \n",
    "        # Pitch shifting\n",
    "        if np.random.random() < 0.5:\n",
    "            n_steps = np.random.uniform(-4, 4)\n",
    "            audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n",
    "        \n",
    "        # Add noise\n",
    "        if np.random.random() < 0.3:\n",
    "            noise = np.random.normal(0, 0.005, audio.shape)\n",
    "            audio = audio + noise\n",
    "        \n",
    "        return audio\n",
    "\n",
    "# Data Quality Assurance\n",
    "class DataQualityChecker:\n",
    "    \"\"\"\n",
    "    Quality assurance for processed datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processed_data_path):\n",
    "        self.processed_data_path = processed_data_path\n",
    "    \n",
    "    def validate_image_data(self, data):\n",
    "        \"\"\"Validate image data quality\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        for i, sample in enumerate(data):\n",
    "            try:\n",
    "                img = Image.open(sample['image_path'])\n",
    "                \n",
    "                # Check image dimensions\n",
    "                if img.size[0] < 64 or img.size[1] < 64:\n",
    "                    issues.append(f\"Image {i} too small: {img.size}\")\n",
    "                \n",
    "                # Check image format\n",
    "                if img.mode not in ['RGB', 'RGBA', 'L']:\n",
    "                    issues.append(f\"Image {i} unsupported mode: {img.mode}\")\n",
    "                \n",
    "                # Check file corruption\n",
    "                img.verify()\n",
    "                \n",
    "            except Exception as e:\n",
    "                issues.append(f\"Image {i} corrupted: {str(e)}\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def validate_audio_data(self, data):\n",
    "        \"\"\"Validate audio data quality\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        for i, sample in enumerate(data):\n",
    "            try:\n",
    "                if 'mel_spectrogram' in sample:\n",
    "                    mel_spec = sample['mel_spectrogram']\n",
    "                    \n",
    "                    # Check spectrogram shape\n",
    "                    if len(mel_spec.shape) != 2:\n",
    "                        issues.append(f\"Audio {i} invalid spectrogram shape: {mel_spec.shape}\")\n",
    "                    \n",
    "                    # Check for NaN values\n",
    "                    if np.isnan(mel_spec).any():\n",
    "                        issues.append(f\"Audio {i} contains NaN values\")\n",
    "                    \n",
    "                    # Check for infinite values\n",
    "                    if np.isinf(mel_spec).any():\n",
    "                        issues.append(f\"Audio {i} contains infinite values\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                issues.append(f\"Audio {i} processing error: {str(e)}\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"Generate comprehensive quality report\"\"\"\n",
    "        report = {\n",
    "            'dataset': [],\n",
    "            'total_samples': [],\n",
    "            'valid_samples': [],\n",
    "            'corrupted_samples': [],\n",
    "            'issues': []\n",
    "        }\n",
    "        \n",
    "        # Check all processed datasets\n",
    "        for file_name in os.listdir(self.processed_data_path):\n",
    "            if file_name.endswith('.csv'):\n",
    "                dataset_name = file_name.replace('.csv', '')\n",
    "                file_path = os.path.join(self.processed_data_path, file_name)\n",
    "                \n",
    "                df = pd.read_csv(file_path)\n",
    "                issues = self.validate_image_data(df.to_dict('records'))\n",
    "                \n",
    "                report['dataset'].append(dataset_name)\n",
    "                report['total_samples'].append(len(df))\n",
    "                report['valid_samples'].append(len(df) - len(issues))\n",
    "                report['corrupted_samples'].append(len(issues))\n",
    "                report['issues'].append(issues[:5])  # First 5 issues\n",
    "        \n",
    "        return pd.DataFrame(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970070d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T15:58:54.532896Z",
     "iopub.status.busy": "2025-07-15T15:58:54.532704Z",
     "iopub.status.idle": "2025-07-15T15:58:54.538536Z",
     "shell.execute_reply": "2025-07-15T15:58:54.537823Z"
    },
    "papermill": {
     "duration": 0.009331,
     "end_time": "2025-07-15T15:58:54.539672",
     "exception": false,
     "start_time": "2025-07-15T15:58:54.530341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiModalDeepfakeDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, data_type='image'):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.data_type = data_type\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = sample['label']\n",
    "        \n",
    "        if self.data_type == 'image':\n",
    "            image = Image.open(sample['image_path']).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        \n",
    "        elif self.data_type == 'video':\n",
    "            frames = sample['frames']\n",
    "            if self.transform:\n",
    "                frames = [self.transform(Image.fromarray(frame)) for frame in frames]\n",
    "            return torch.stack(frames), label\n",
    "        \n",
    "        elif self.data_type == 'audio':\n",
    "            mel_spec = sample['mel_spectrogram']\n",
    "            mel_spec = torch.FloatTensor(mel_spec).unsqueeze(0)\n",
    "            return mel_spec, label\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data type: {self.data_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5368f9bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T15:58:54.544004Z",
     "iopub.status.busy": "2025-07-15T15:58:54.543778Z",
     "iopub.status.idle": "2025-07-15T17:19:28.576052Z",
     "shell.execute_reply": "2025-07-15T17:19:28.575070Z"
    },
    "papermill": {
     "duration": 4834.035852,
     "end_time": "2025-07-15T17:19:28.577404",
     "exception": false,
     "start_time": "2025-07-15T15:58:54.541552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive dataset processing...\n",
      "================================================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing FF-C23 dataset...\n",
      "FF-C23 processed: 5000 videos\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing CelebDF-v2 dataset...\n",
      "CelebDF-v2 processed: 101031 images\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing ASVspoof-21 CQT dataset...\n",
      "ASVspoof-21 CQT processed: 611828 images\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing DFDC faces dataset...\n",
      "DFDC faces processed: 124647 images\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing In-The-Wild audio deepfake dataset...\n",
      "In-The-Wild audio processed: 31779 samples\n",
      "\n",
      "================================================================================\n",
      "DATASET PROCESSING SUMMARY\n",
      "================================================================================\n",
      "          Dataset  Status  Total_Samples  Real_Samples  Fake_Samples Data_Type\n",
      "           ff_c23 Success           5000          1000          4000     Video\n",
      "       celebdf_v2 Success         101031         50360         50671     Image\n",
      "  asvspoof_21_cqt Success         611828         22617        589211     Image\n",
      "       dfdc_faces Success         124647         26728         97919     Image\n",
      "in_the_wild_audio Success          31779         19963         11816     Audio\n",
      "\n",
      "Summary saved to: /kaggle/working/processed_data/processing_summary.csv\n",
      "\n",
      "Phase 2: Data Collection and Preparation completed successfully!\n",
      "Processed datasets are saved in: /kaggle/working/processed_data\n"
     ]
    }
   ],
   "source": [
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    dataset_manager = RobustMultiModalDatasetManager()\n",
    "    \n",
    "    # Process all datasets\n",
    "    results = dataset_manager.process_all_datasets()\n",
    "    \n",
    "    print(\"\\nPhase 2: Data Collection and Preparation completed successfully!\")\n",
    "    print(\"Processed datasets are saved in:\", dataset_manager.processed_data_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 493642,
     "sourceId": 979785,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4836275,
     "sourceId": 8171572,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5549800,
     "sourceId": 9181939,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6248577,
     "sourceId": 10125851,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6635046,
     "sourceId": 10705927,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4854.29007,
   "end_time": "2025-07-15T17:19:31.693778",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-15T15:58:37.403708",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
